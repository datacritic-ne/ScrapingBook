{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43560ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 33) (853141281.py, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 33\u001b[0;36m\u001b[0m\n\u001b[0;31m    url = 'https://www.cnn.com/2023/04/03/investing/\\'\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 33)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "class Content:\n",
    "    def __init__(self, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "    def print(self):\n",
    "        print(f'TITLE: {self.title}')\n",
    "        print(f'URL: {self.url}')\n",
    "        print(f'BODY: {self.body}')\n",
    "        \n",
    "def scrapeCNN(url):\n",
    "    bs = BeautifulSoup(urlopen(url))\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class': 'article_content'}).text\n",
    "    print('body: ')\n",
    "    print(body)\n",
    "    return Content(url, title, body)\n",
    "\n",
    "def scrapeBrookings(url):\n",
    "    bs = BeautifulSoup(urlopen(url))\n",
    "    title = bs.find('h1').text\n",
    "    body = bs.find('div', {'class': 'post-body'}).text\n",
    "    return Content(url, title, body)\n",
    "\n",
    "url = 'https://www.brookings.edu/research/robotic-rulemaking'\n",
    "content = scrapeBrookings(url)\n",
    "content.print()\n",
    "\n",
    "url = 'https://www.cnn.com/2023/04/03/investing/\\\n",
    "dogecoin-elon-musk-twitter/index.html'\n",
    "content = scrapeCNN(url)\n",
    "content.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3446f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles or pages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url, title, body):\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function to control output\n",
    "        \"\"\"\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "        \n",
    "class Website:\n",
    "    \"\"\"\n",
    "    Contains information about website structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, url, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6edcfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    \n",
    "    def getPage(url):\n",
    "        try:\n",
    "            html = urlopen(url)\n",
    "        except Exception:\n",
    "            return None\n",
    "        return BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    def safeGet(bs, selector):\n",
    "        \"\"\"\n",
    "        Utility function used to get content string from a Beautiful Soup \n",
    "        object and a selector. Returns an empty string if no object is found\n",
    "        for the given selector.\n",
    "        \"\"\"\n",
    "        selectedElems = bs.select(selector)\n",
    "        if selectedElems is not None and len(selectedElems) > 0:\n",
    "            return '\\n'.join([elem.get_text() for elem in selectedElems])\n",
    "        return ''\n",
    "    \n",
    "    def getContent(website, path):\n",
    "        \"\"\"\n",
    "        Pull content from a given URL\n",
    "        \"\"\"\n",
    "        url = website.url+path\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs.website.titleTag)\n",
    "            body = Crawler.safeGet(bs.website.bodyTag)\n",
    "            return Content(url, title, body)\n",
    "        return Content(url, '', '')\n",
    "    \n",
    "    siteData = [\n",
    "        ['O\\'Reilly', 'https://www.oreilly.com', 'h1', 'div.title-description'],\n",
    "        ['Reuters', 'https://www.reuters.com', 'h1', 'div.ArticleBodyWrapper'],\n",
    "        ['Brookings', 'https://www.brookings.edu', 'h1', 'div.post-body'],\n",
    "        ['CNN', 'https://www.cnn.com', 'h1', 'div.article_content']\n",
    "    ]\n",
    "    websites = []\n",
    "    for name, url, title, body in siteData:\n",
    "        websites.append(Website(name, url, title, body))\n",
    "        \n",
    "    Crawler.getContent(\n",
    "        websites[0],\n",
    "        '/library/view/web-scraping-with/9781491910283'\n",
    "        ).print()\n",
    "    Crawler.getContent(\n",
    "        websites[1],\n",
    "        '/article/us-usa-epa-pruitt-idUSKBN19W2D0'\n",
    "        ).print()\n",
    "    Crawler.getContent(\n",
    "        websites[2],\n",
    "        '/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/'\n",
    "        ).print()\n",
    "    Crawler.getContent(\n",
    "        websites[3],\n",
    "        '/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html'\n",
    "        ).print()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4fbd49af",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdc8236",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Content:\n",
    "    \"\"\"\n",
    "    Common base class for all articles or pages\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, topic, url, title, body):\n",
    "        self.topic = topic\n",
    "        self.url = url\n",
    "        self.title = title\n",
    "        self.body = body\n",
    "        \n",
    "    def print(self):\n",
    "        \"\"\"\n",
    "        Flexible printing function to control output\n",
    "        \"\"\"\n",
    "        print('New article found for topic: {}'.format(self.topic))\n",
    "        print('URL: {}'.format(self.url))\n",
    "        print('TITLE: {}'.format(self.title))\n",
    "        print('BODY:\\n{}'.format(self.body))\n",
    "        \n",
    "class Website:\n",
    "    \"\"\"\n",
    "    Contains information about website structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, url, searchUrl, resultListing,\n",
    "        resultUrl, absoluteUrl, titleTag, bodyTag):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "        self.searchUrl = searchUrl\n",
    "        self.resultListing = resultListing\n",
    "        self.resultUrl = resultUrl\n",
    "        self.absoluteUrl = absoluteUrl\n",
    "        self.titleTag = titleTag\n",
    "        self.bodyTag = bodyTag\n",
    "        \n",
    "class Crawler:\n",
    "    \n",
    "    def __init__(self, website):\n",
    "        self.site = website\n",
    "        self.found = {}\n",
    "        \n",
    "    def getContent(self, topic, url):\n",
    "        \"\"\"\n",
    "        Pull content from a given URL\n",
    "        \"\"\"\n",
    "       # url = website.url+path\n",
    "        bs = Crawler.getPage(url)\n",
    "        if bs is not None:\n",
    "            title = Crawler.safeGet(bs, self.site.titleTag)\n",
    "            body = Crawler.safeGet(bs, self.site.bodyTag)\n",
    "            return Content(topic, url, title, body)\n",
    "        return Content(topic, url, '', '')\n",
    "    \n",
    "    def search(self, topic):\n",
    "        \"\"\"\n",
    "        Searches a given site for a given topic and records all pages found.\n",
    "        \"\"\"\n",
    "        bs = Crawler.getPage(self.site.searchUrl + topic)\n",
    "        searchResults = bs.select(self.site.resultListing)\n",
    "        for result in searchResults:\n",
    "            url = result.select(self.site.resultUrl)[0].attrs['href']\n",
    "            # Is this a relative or absolute URL?\n",
    "            url = url if self.site.absoluteUrl else self.site.url + url\n",
    "            if url not in self.found:\n",
    "                self.found[url] = self.getContent(topic, url)\n",
    "            self.found[url].print()\n",
    "            \n",
    "siteData = [\n",
    "    ['Reuters', 'http://www.reuters.com', \n",
    "    'https://www.reuters.com/search/news?blob=',\n",
    "    'div.search-result-indiv', 'h3.search-result-title a',\n",
    "    False, 'h1', 'div.ArticleBodyWrapper'],\n",
    "    ['Brookings', 'http://www.brookings.edu',\n",
    "    'https://www.brookings.edu/search/?s=',\n",
    "    'div.article-info', 'h4.title a', True, 'h1', 'div.core-block']\n",
    "]\n",
    "sites = []\n",
    "for name, url, search, rListing, rUrl, absUrl, tt, bt in siteData:\n",
    "    sites.append(Website(name, url, search, rListing, rUrl, absUrl, tt, bt))\n",
    "    \n",
    "crawlers = [Crawler(site) for site in sites]\n",
    "topics = ['python', 'data%20science']\n",
    "\n",
    "for topic in topics:\n",
    "    for crawler in crawlers:\n",
    "        crawler.search(topic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
